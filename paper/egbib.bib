@article{taketomi_visual_2017,
	title = {Visual {SLAM} algorithms: a survey from 2010 to 2016},
	volume = {9},
	issn = {1882-6695},
	shorttitle = {Visual {SLAM} algorithms},
	url = {http://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2},
	doi = {10.1186/s41074-017-0027-2},
	language = {en},
	number = {1},
	urldate = {2018-02-22},
	journal = {IPSJ Transactions on Computer Vision and Applications},
	author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
	month = dec,
	year = {2017},
	file = 
{s41074-017-0027-2.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/3MJ335NE/s41074-017-0027-2.pdf:application/pdf}
}

@inproceedings{wang_survey_2017,
	title = {A survey of simultaneous localization and mapping on unstructured lunar complex environment},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.5005198},
	doi = {10.1063/1.5005198},
	urldate = {2018-02-22},
	author = {Wang, Yiqiao and Zhang, Wei and An, Pei},
	year = {2017},
	pages = {030010},
	file = 
{1.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/EN54IT4J/1.pdf:application/pdf}
}


@inproceedings{zhang_loop_2017,
	title = {Loop closure detection for visual {SLAM} systems using convolutional neural network},
	doi = {10.23919/IConAC.2017.8082072},
	abstract = {This paper is concerned of the loop closure detection problem, which is one of the most 
critical parts for visual Simultaneous Localization and Mapping (SLAM) systems. Most of state-of-the-art 
methods use hand-crafted features and bag-of-visual-words (BoVW) to tackle this problem. Recent development in 
deep learning indicates that CNN features significantly outperform hand-crafted features for image 
representation. This advanced technology has not been fully exploited in robotics, especially in visual SLAM 
systems. We propose a loop closure detection method based on convolutional neural networks (CNNs). Images are 
fed into a pre-trained CNN model to extract features. We pre-process CNN features instead of using them 
directly as most of the presented approaches did before they are used to detect loops. The workflow of 
extracting CNN features, processing data, computing similarity score and detecting loops is presented. Finally 
the performance of proposed method is evaluated on several open datasets by comparing it with Fab-Map using 
precision-recall metric.},
	booktitle = {2017 23rd {International} {Conference} on {Automation} and {Computing} ({ICAC})},
	author = {Zhang, X. and Su, Y. and Zhu, X.},
	month = sep,
	year = {2017},
	keywords = {learning (artificial intelligence), feature extraction, Principal component analysis, 
Visualization, Feature extraction, image representation, Deep Learning, Neural networks, bag-of-visual-words, 
CNN model, Computational modeling, convolution, convolutional neural network, Convolutional Neural Network, 
data processing, feedforward neural nets, hand-crafted features, Image representation, Loop Closure Detection, 
loop closure detection method, loop closure detection problem, mobile robots, pre-process CNN features, 
precision-recall metric, robot vision, similarity score, Simultaneous localization and mapping, SLAM, SLAM 
(robots), visual simultaneous localization and mapping systems, visual SLAM systems},
	pages = {1--6},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/59XH5I3S/8082072.html:text/html}
}

@article{taketomi_visual_2017,
	title = {Visual {SLAM} algorithms: a survey from 2010 to 2016},
	volume = {9},
	issn = {1882-6695},
	shorttitle = {Visual {SLAM} algorithms},
	url = {http://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2},
	doi = {10.1186/s41074-017-0027-2},
	language = {en},
	number = {1},
	urldate = {2018-02-22},
	journal = {IPSJ Transactions on Computer Vision and Applications},
	author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
	month = dec,
	year = {2017},
	file = 
{s41074-017-0027-2.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/3MJ335NE/s41074-017-0027-2.pdf:application/pdf}
}

@article{gupta_cognitive_2017,
	title = {Cognitive mapping and planning for visual navigation},
	volume = {3},
	journal = {arXiv preprint arXiv:1702.03920},
	author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, 
Jitendra},
	year = {2017},
	file = 
{Gupta_Cognitive_Mapping_and_CVPR_2017_paper.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/HTW6Q3BM/Gupta_Cognitive_Mapping_and_CVPR_2017_paper.pdf:application/pdf}
}

@article{gao_unsupervised_2017,
	title = {Unsupervised learning to detect loops using deep neural networks for visual {SLAM} system},
	volume = {41},
	issn = {0929-5593, 1573-7527},
	url = {http://link.springer.com/10.1007/s10514-015-9516-2},
	doi = {10.1007/s10514-015-9516-2},
	language = {en},
	number = {1},
	urldate = {2018-02-22},
	journal = {Autonomous Robots},
	author = {Gao, Xiang and Zhang, Tao},
	month = jan,
	year = {2017},
	pages = {1--18},
	file = 
{10.1007s10514-015-9516-2.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/JTMGU2H3/10.1007s10514-015-9516-2.pdf:application/pdf}
}

@inproceedings{wang_survey_2017,
	title = {A survey of simultaneous localization and mapping on unstructured lunar complex environment},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.5005198},
	doi = {10.1063/1.5005198},
	urldate = {2018-02-22},
	author = {Wang, Yiqiao and Zhang, Wei and An, Pei},
	year = {2017},
	pages = {030010},
	file = 
{1.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/EN54IT4J/1.pdf:application/pdf}
}

@article{naseer_robust_2018,
	title = {Robust {Visual} {Localization} {Across} {Seasons}},
	volume = {PP},
	issn = {1552-3098},
	doi = {10.1109/TRO.2017.2788045},
	abstract = {Localization is an integral part of reliable robot navigation, and long-term autonomy 
requires robustness against perceptional changes in the environment during localization. In the context of 
vision-based localization, such changes can be caused by illumination variations, occlusion, structural 
development, different weather conditions, and seasons. In this paper, we present a novel approach for 
localizing a robot over longer periods of time using only monocular image data. We propose a novel data 
association approach for matching streams of incoming images to an image sequence stored in a database. Our 
method exploits network flows to leverage sequential information to improve the localization performance and 
to maintain several possible trajectories hypotheses in parallel. To compare images, we consider a semidense 
image description based on histogram of oriented gradients features as well as global descriptors from deep 
convolutional neural networks trained on ImageNet for robust localization. We perform extensive evaluations on 
a variety of datasets and show that our approach outperforms existing state-of-the-art approaches.},
	number = {99},
	journal = {IEEE Transactions on Robotics},
	author = {Naseer, T. and Burgard, W. and Stachniss, C.},
	year = {2018},
	keywords = {Robustness, Visualization, Image matching, Image sequences, Lighting, Meteorology, 
Robots},
	pages = {1--14},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/BB9BXT4I/8269404.html:text/html}
}

@inproceedings{pascoe_nid-slam:_2017,
	title = {{NID}-{SLAM}: {Robust} {Monocular} {SLAM} using {Normalised} {Information} {Distance}},
	shorttitle = {{NID}-{SLAM}},
	booktitle = {Conference on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pascoe, Geoffrey and Maddern, Will and Tanner, Michael and Piniés, Pedro and Newman, Paul},
	year = {2017},
	file = 
{Pascoe_NID-SLAM_Robust_Monocular_CVPR_2017_paper.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/V8ZSZTIS/Pascoe_NID-SLAM_Robust_Monocular_CVPR_2017_paper.pdf:application/pdf}
}

@inproceedings{naseer_robust_2014,
	title = {Robust {Visual} {Robot} {Localization} {Across} {Seasons} {Using} {Network} {Flows}.},
	booktitle = {{AAAI}},
	author = {Naseer, Tayyab and Spinello, Luciano and Burgard, Wolfram and Stachniss, Cyrill},
	year = {2014},
	pages = {2564--2570},
	file = 
{naseerAAAI14.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/BTMNUGKU/naseerAAAI14.pdf:application/pdf}
}

@article{cummins_appearance-only_2011,
	title = {Appearance-only {SLAM} at large scale with {FAB}-{MAP} 2.0},
	volume = {30},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364910385483},
	doi = {10.1177/0278364910385483},
	abstract = {We describe a new formulation of appearance-only SLAM suitable for very large                     
scale place recognition. The system navigates in the space of appearance,                     assigning each 
new observation to either a new or a previously visited location,                     without reference to 
metric position. The system is demonstrated performing                     reliable online appearance mapping 
and loop-closure detection over a 1000 km                     trajectory, with mean filter update times of 14 
ms. The scalability of the                     system is achieved by defining a sparse approximation to the 
FAB-MAP model                     suitable for implementation using an inverted index. Our formulation of the                     
problem is fully probabilistic and naturally incorporates robustness against                     perceptual 
aliasing. We also demonstrate that the approach substantially                     outperforms the standard 
term-frequency inverse-document-frequency (tf-idf)                     ranking measure. The 1000 km data set 
comprising almost a terabyte of                     omni-directional and stereo imagery is available for use, 
and we hope that it                     will serve as a benchmark for future systems.},
	language = {en},
	number = {9},
	urldate = {2018-03-18},
	journal = {The International Journal of Robotics Research},
	author = {Cummins, Mark and Newman, Paul},
	month = aug,
	year = {2011},
	pages = {1100--1123}
}

@inproceedings{hou_convolutional_2015,
	title = {Convolutional neural network-based image representation for visual loop closure detection},
	doi = {10.1109/ICInfA.2015.7279659},
	abstract = {Deep convolutional neural networks (CNN) have recently been shown in many computer vision 
and pattern recognition applications to outperform by a significant margin state-of-the-art solutions that use 
traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in 
robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN 
technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation 
appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a 
comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in 
comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting 
loop closures. The main conclusions of our study include: (a) CNN-based image representations perform 
comparably to state-of-the-art hand-crafted competitors in environments without significant lighting change, 
(b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also 
significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and 
are two orders of magnitude faster on an entry-level GPU.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Information} and {Automation}},
	author = {Hou, Y. and Zhang, H. and Zhou, S.},
	month = aug,
	year = {2015},
	keywords = {neural nets, object detection, Standards, Visualization, Feature extraction, image 
representation, convolutional neural network, robot vision, Simultaneous localization and mapping, SLAM, SLAM 
(robots), Lighting, CNN technology, computer vision application, conventional CPU, Convolutional neural 
networks(CNN), entry-level GPU, graphics processing unit, hand-crafted feature, image descriptors, image 
matching, Image retrieval, loop closure detection, pattern recognition application, robotics, simultaneous 
localization and mapping, visual loop closure detection},
	pages = {2238--2245}
}

@inproceedings{naseer_robust_2015,
	title = {Robust visual {SLAM} across seasons},
	doi = {10.1109/IROS.2015.7353721},
	abstract = {In this paper, we present an appearance-based visual SLAM approach that focuses on 
detecting loop closures across seasons. Given two image sequences, our method first extracts one descriptor 
per image for both sequences using a deep convolutional neural network. Then, we compute a similarity matrix 
by comparing each image of a query sequence with a database. Finally, based on the similarity matrix, we 
formulate a flow network problem and compute matching hypotheses between sequences. In this way, our approach 
can handle partially matching routes, loops in the trajectory and different speeds of the robot. With a 
matching hypothesis as loop closure information and the odometry information of the robot, we formulate a 
graph based SLAM problem and compute a joint maximum likelihood trajectory.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} 
({IROS})},
	author = {Naseer, T. and Ruhnke, M. and Stachniss, C. and Spinello, L. and Burgard, W.},
	month = sep,
	year = {2015},
	keywords = {appearance-based visual SLAM approach, Databases, deep convolutional neural network, 
descriptor extraction, feature extraction, Feature extraction, flow network problem, graph based SLAM problem, 
image matching, image retrieval, image sequences, joint maximum likelihood trajectory, loop-closure detection, 
matching hypothesis, matrix algebra, neural nets, odometry information, partially matching routes, query 
sequence, robot vision, robust visual SLAM, Robustness, similarity matrix, Simultaneous localization and 
mapping, SLAM (robots), Trajectory, Visualization},
	pages = {2529--2535},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/IGPB9WXX/7353721.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/QNLHRMMX/Naseer et al. - 2015 - Robust 
visual SLAM across seasons.pdf:application/pdf}
}


@inproceedings{zhang_loop_2017,
	title = {Loop closure detection for visual {SLAM} systems using convolutional neural network},
	doi = {10.23919/IConAC.2017.8082072},
	abstract = {This paper is concerned of the loop closure detection problem, which is one of the most 
critical parts for visual Simultaneous Localization and Mapping (SLAM) systems. Most of state-of-the-art 
methods use hand-crafted features and bag-of-visual-words (BoVW) to tackle this problem. Recent development in 
deep learning indicates that CNN features significantly outperform hand-crafted features for image 
representation. This advanced technology has not been fully exploited in robotics, especially in visual SLAM 
systems. We propose a loop closure detection method based on convolutional neural networks (CNNs). Images are 
fed into a pre-trained CNN model to extract features. We pre-process CNN features instead of using them 
directly as most of the presented approaches did before they are used to detect loops. The workflow of 
extracting CNN features, processing data, computing similarity score and detecting loops is presented. Finally 
the performance of proposed method is evaluated on several open datasets by comparing it with Fab-Map using 
precision-recall metric.},
	booktitle = {2017 23rd {International} {Conference} on {Automation} and {Computing} ({ICAC})},
	author = {Zhang, X. and Su, Y. and Zhu, X.},
	month = sep,
	year = {2017},
	keywords = {learning (artificial intelligence), feature extraction, Principal component analysis, 
Visualization, Feature extraction, image representation, Deep Learning, Neural networks, bag-of-visual-words, 
CNN model, Computational modeling, convolution, convolutional neural network, Convolutional Neural Network, 
data processing, feedforward neural nets, hand-crafted features, Image representation, Loop Closure Detection, 
loop closure detection method, loop closure detection problem, mobile robots, pre-process CNN features, 
precision-recall metric, robot vision, similarity score, Simultaneous localization and mapping, SLAM, SLAM 
(robots), visual simultaneous localization and mapping systems, visual SLAM systems},
	pages = {1--6},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/59XH5I3S/8082072.html:text/html}
}

@article{taketomi_visual_2017,
	title = {Visual {SLAM} algorithms: a survey from 2010 to 2016},
	volume = {9},
	issn = {1882-6695},
	shorttitle = {Visual {SLAM} algorithms},
	url = {http://ipsjcva.springeropen.com/articles/10.1186/s41074-017-0027-2},
	doi = {10.1186/s41074-017-0027-2},
	language = {en},
	number = {1},
	urldate = {2018-02-22},
	journal = {IPSJ Transactions on Computer Vision and Applications},
	author = {Taketomi, Takafumi and Uchiyama, Hideaki and Ikeda, Sei},
	month = dec,
	year = {2017},
	file = 
{s41074-017-0027-2.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/3MJ335NE/s41074-017-0027-2.pdf:application/pdf}
}

@article{gupta_cognitive_2017,
	title = {Cognitive mapping and planning for visual navigation},
	volume = {3},
	journal = {arXiv preprint arXiv:1702.03920},
	author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, 
Jitendra},
	year = {2017},
	file = 
{Gupta_Cognitive_Mapping_and_CVPR_2017_paper.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/HTW6Q3BM/Gupta_Cognitive_Mapping_and_CVPR_2017_paper.pdf:application/pdf}
}

@article{gao_unsupervised_2017,
	title = {Unsupervised learning to detect loops using deep neural networks for visual {SLAM} system},
	volume = {41},
	issn = {0929-5593, 1573-7527},
	url = {http://link.springer.com/10.1007/s10514-015-9516-2},
	doi = {10.1007/s10514-015-9516-2},
	language = {en},
	number = {1},
	urldate = {2018-02-22},
	journal = {Autonomous Robots},
	author = {Gao, Xiang and Zhang, Tao},
	month = jan,
	year = {2017},
	pages = {1--18},
	file = 
{10.1007s10514-015-9516-2.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/JTMGU2H3/10.1007s10514-015-9516-2.pdf:application/pdf}
}

@inproceedings{wang_survey_2017,
	title = {A survey of simultaneous localization and mapping on unstructured lunar complex environment},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.5005198},
	doi = {10.1063/1.5005198},
	urldate = {2018-02-22},
	author = {Wang, Yiqiao and Zhang, Wei and An, Pei},
	year = {2017},
	pages = {030010},
	file = 
{1.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/EN54IT4J/1.pdf:application/pdf}
}

@article{naseer_robust_2018,
	title = {Robust {Visual} {Localization} {Across} {Seasons}},
	volume = {PP},
	issn = {1552-3098},
	doi = {10.1109/TRO.2017.2788045},
	abstract = {Localization is an integral part of reliable robot navigation, and long-term autonomy 
requires robustness against perceptional changes in the environment during localization. In the context of 
vision-based localization, such changes can be caused by illumination variations, occlusion, structural 
development, different weather conditions, and seasons. In this paper, we present a novel approach for 
localizing a robot over longer periods of time using only monocular image data. We propose a novel data 
association approach for matching streams of incoming images to an image sequence stored in a database. Our 
method exploits network flows to leverage sequential information to improve the localization performance and 
to maintain several possible trajectories hypotheses in parallel. To compare images, we consider a semidense 
image description based on histogram of oriented gradients features as well as global descriptors from deep 
convolutional neural networks trained on ImageNet for robust localization. We perform extensive evaluations on 
a variety of datasets and show that our approach outperforms existing state-of-the-art approaches.},
	number = {99},
	journal = {IEEE Transactions on Robotics},
	author = {Naseer, T. and Burgard, W. and Stachniss, C.},
	year = {2018},
	keywords = {Robustness, Visualization, Image matching, Image sequences, Lighting, Meteorology, 
Robots},
	pages = {1--14},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/BB9BXT4I/8269404.html:text/html}
}

@inproceedings{pascoe_nid-slam:_2017,
	title = {{NID}-{SLAM}: {Robust} {Monocular} {SLAM} using {Normalised} {Information} {Distance}},
	shorttitle = {{NID}-{SLAM}},
	booktitle = {Conference on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pascoe, Geoffrey and Maddern, Will and Tanner, Michael and Piniés, Pedro and Newman, Paul},
	year = {2017},
	file = 
{Pascoe_NID-SLAM_Robust_Monocular_CVPR_2017_paper.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/V8ZSZTIS/Pascoe_NID-SLAM_Robust_Monocular_CVPR_2017_paper.pdf:application/pdf}
}

@inproceedings{naseer_robust_2014,
	title = {Robust {Visual} {Robot} {Localization} {Across} {Seasons} {Using} {Network} {Flows}.},
	booktitle = {{AAAI}},
	author = {Naseer, Tayyab and Spinello, Luciano and Burgard, Wolfram and Stachniss, Cyrill},
	year = {2014},
	pages = {2564--2570},
	file = 
{naseerAAAI14.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/BTMNUGKU/naseerAAAI14.pdf:application/pdf}
}

@article{cummins_appearance-only_2011,
	title = {Appearance-only {SLAM} at large scale with {FAB}-{MAP} 2.0},
	volume = {30},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364910385483},
	doi = {10.1177/0278364910385483},
	abstract = {We describe a new formulation of appearance-only SLAM suitable for very large                     
scale place recognition. The system navigates in the space of appearance,                     assigning each 
new observation to either a new or a previously visited location,                     without reference to 
metric position. The system is demonstrated performing                     reliable online appearance mapping 
and loop-closure detection over a 1000 km                     trajectory, with mean filter update times of 14 
ms. The scalability of the                     system is achieved by defining a sparse approximation to the 
FAB-MAP model                     suitable for implementation using an inverted index. Our formulation of the                     
problem is fully probabilistic and naturally incorporates robustness against                     perceptual 
aliasing. We also demonstrate that the approach substantially                     outperforms the standard 
term-frequency inverse-document-frequency (tf-idf)                     ranking measure. The 1000 km data set 
comprising almost a terabyte of                     omni-directional and stereo imagery is available for use, 
and we hope that it                     will serve as a benchmark for future systems.},
	language = {en},
	number = {9},
	urldate = {2018-03-18},
	journal = {The International Journal of Robotics Research},
	author = {Cummins, Mark and Newman, Paul},
	month = aug,
	year = {2011},
	pages = {1100--1123}
}

@inproceedings{hou_convolutional_2015,
	title = {Convolutional neural network-based image representation for visual loop closure detection},
	doi = {10.1109/ICInfA.2015.7279659},
	abstract = {Deep convolutional neural networks (CNN) have recently been shown in many computer vision 
and pattern recognition applications to outperform by a significant margin state-of-the-art solutions that use 
traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in 
robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN 
technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation 
appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a 
comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in 
comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting 
loop closures. The main conclusions of our study include: (a) CNN-based image representations perform 
comparably to state-of-the-art hand-crafted competitors in environments without significant lighting change, 
(b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also 
significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and 
are two orders of magnitude faster on an entry-level GPU.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Information} and {Automation}},
	author = {Hou, Y. and Zhang, H. and Zhou, S.},
	month = aug,
	year = {2015},
	keywords = {neural nets, object detection, Standards, Visualization, Feature extraction, image 
representation, convolutional neural network, robot vision, Simultaneous localization and mapping, SLAM, SLAM 
(robots), Lighting, CNN technology, computer vision application, conventional CPU, Convolutional neural 
networks(CNN), entry-level GPU, graphics processing unit, hand-crafted feature, image descriptors, image 
matching, Image retrieval, loop closure detection, pattern recognition application, robotics, simultaneous 
localization and mapping, visual loop closure detection},
	pages = {2238--2245},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/V9UMRSXL/7279659.html:text/html}
}

@misc{noauthor_download_nodate,
	title = {Download {New} {Tsukuba} {Stereo} {Dataset}},
	url = {http://www.cvlab.cs.tsukuba.ac.jp/dataset/tsukubastereo.php},
	urldate = {2018-03-18},
	file = {Download New Tsukuba Stereo 
Dataset:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/8JHXHY4T/tsukubastereo.html:text/html}
}

@inproceedings{naseer_robust_2015,
	title = {Robust visual {SLAM} across seasons},
	doi = {10.1109/IROS.2015.7353721},
	abstract = {In this paper, we present an appearance-based visual SLAM approach that focuses on 
detecting loop closures across seasons. Given two image sequences, our method first extracts one descriptor 
per image for both sequences using a deep convolutional neural network. Then, we compute a similarity matrix 
by comparing each image of a query sequence with a database. Finally, based on the similarity matrix, we 
formulate a flow network problem and compute matching hypotheses between sequences. In this way, our approach 
can handle partially matching routes, loops in the trajectory and different speeds of the robot. With a 
matching hypothesis as loop closure information and the odometry information of the robot, we formulate a 
graph based SLAM problem and compute a joint maximum likelihood trajectory.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} 
({IROS})},
	author = {Naseer, T. and Ruhnke, M. and Stachniss, C. and Spinello, L. and Burgard, W.},
	month = sep,
	year = {2015},
	keywords = {neural nets, feature extraction, Robustness, Visualization, Feature extraction, robot 
vision, Simultaneous localization and mapping, SLAM (robots), image matching, appearance-based visual SLAM 
approach, Databases, deep convolutional neural network, descriptor extraction, flow network problem, graph 
based SLAM problem, image retrieval, image sequences, joint maximum likelihood trajectory, loop-closure 
detection, matching hypothesis, matrix algebra, odometry information, partially matching routes, query 
sequence, robust visual SLAM, similarity matrix, Trajectory},
	pages = {2529--2535},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/IGPB9WXX/7353721.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/QNLHRMMX/Naseer et al. - 2015 - Robust 
visual SLAM across seasons.pdf:application/pdf}
}

@inproceedings{klein_parallel_2007,
	title = {Parallel tracking and mapping for small {AR} workspaces},
	booktitle = {Mixed and {Augmented} {Reality}, 2007. {ISMAR} 2007. 6th {IEEE} and {ACM} {International} 
{Symposium} on},
	publisher = {IEEE},
	author = {Klein, Georg and Murray, David},
	year = {2007},
	pages = {225--234},
	file = 
{KleinMurray2007ISMAR.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/Q4WL9U57/KleinMurray2007ISMAR.pdf:application/pdf}
}

@article{mur-artal_orb-slam:_2015,
	title = {{ORB}-{SLAM}: {A} {Versatile} and {Accurate} {Monocular} {SLAM} {System}},
	volume = {31},
	issn = {1552-3098},
	shorttitle = {{ORB}-{SLAM}},
	doi = {10.1109/TRO.2015.2463671},
	abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and 
mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The 
system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes 
full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a 
novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop 
closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads 
to excellent robustness and generates a compact and trackable map that only grows if the scene content 
changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most 
popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular 
SLAM approaches. For the benefit of the community, we make the source code public.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Mur-Artal, R. and Montiel, J. M. M. and Tardós, J. D.},
	month = oct,
	year = {2015},
	keywords = {Visualization, Feature extraction, Computational modeling, Simultaneous localization and 
mapping, SLAM (robots), Cameras, feature-based monocular simultaneous localization and mapping system, 
Lifelong mapping, localization, monocular vision, Optimization, ORB-SLAM system, Real-time systems, 
recognition, simultaneous localization and mapping (SLAM), survival of the fittest strategy},
	pages = {1147--1163},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/659EIHPJ/7219438.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/NSUYHISU/Mur-Artal et al. - 2015 - 
ORB-SLAM A Versatile and Accurate Monocular SLAM .pdf:application/pdf}
}

@inproceedings{martinez-carranza_towards_2015,
	title = {Towards autonomous flight of micro aerial vehicles using {ORB}-{SLAM}},
	doi = {10.1109/RED-UAS.2015.7441013},
	abstract = {In the last couple of years a novel visual simultaneous localisation and mapping (SLAM) 
system, based on visual features, has emerged as one of the best, if not the best, systems for estimating the 
6D camera pose whilst building a 3D map of the observed scene. This method is called ORB-SLAM and one of its 
key ideas is to use the same visual descriptor, a binary descriptor called ORB, for all the visual tasks, this 
is, for feature matching, relocalisation and loop closure. On the top of this, ORB-SLAM combines local and 
graph-based global bundle adjustment, which enables a scalable map generation whilst keeping real-time 
performance. Therefore, motivated by its performance in terms of processing speed, robustness against erratic 
motion and scalability, in this paper we present an implementation of autonomous flight for a low-cost micro 
aerial vehicle (MAV), where ORB-SLAM is used as a visual positioning system that feeds a PD controller that 
controls pitch, roll and yaw. Our results indicate that our implementation has potential and could soon be 
implemented on a bigger aerial platform with more complex trajectories to be flown autonomously.},
	booktitle = {2015 {Workshop} on {Research}, {Education} and {Development} of {Unmanned} {Aerial} 
{Systems} ({RED}-{UAS})},
	author = {Martínez-Carranza, J. and Loewen, N. and Márquez, F. and García, E. O. and Mayol-Cuevas, 
W.},
	month = nov,
	year = {2015},
	keywords = {Visualization, Simultaneous localization and mapping, SLAM (robots), image matching, 
Trajectory, Cameras, 3D map, 6D camera pose estimation, autonomous aerial vehicles, autonomous flight, binary 
descriptor, cameras, erratic motion, feature matching, feature relocalisation, graph theory, graph-based 
global bundle adjustment, local bundle adjustment, loop closure, microaerial vehicles, microrobots, 
Navigation, ORB-SLAM, PD control, PD controller, pitch control, pose estimation, position control, processing 
speed, robust control, robustness, roll control, scalable map generation, simultaneous localisation and 
mapping system, Vehicles, visual descriptor, visual features, visual positioning system, yaw control},
	pages = {241--248},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/9L3ET2PX/7441013.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/23GCDADZ/Martínez-Carranza et al. - 
2015 - Towards autonomous flight of micro aerial vehicles.pdf:application/pdf}
}

@inproceedings{engel_lsd-slam:_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{LSD}-{SLAM}: {Large}-{Scale} {Direct} {Monocular} {SLAM}},
	isbn = {978-3-319-10604-5 978-3-319-10605-2},
	shorttitle = {{LSD}-{SLAM}},
	url = {https://link-springer-com.ezproxy.rit.edu/chapter/10.1007/978-3-319-10605-2_54},
	doi = {10.1007/978-3-319-10605-2_54},
	abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current 
state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. 
Along with highly accurate pose estimation based on direct image alignment, the 3D environment is 
reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are 
obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly 
scale-drift aware formulation allows the approach to operate on challenging sequences including large 
variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which 
operates on sim(3){\textbackslash}mathfrak\{sim\}(3), thereby explicitly detecting scale-drift, and (2) an 
elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct 
monocular SLAM system runs in real-time on a CPU.},
	language = {en},
	urldate = {2018-03-27},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer, Cham},
	author = {Engel, Jakob and Schöps, Thomas and Cremers, Daniel},
	month = sep,
	year = {2014},
	pages = {834--849},
	file = {Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/VSG3E5Z7/Engel et al. 
- 2014 - LSD-SLAM Large-Scale Direct Monocular 
SLAM.pdf:application/pdf;Snapshot:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/7HUHDQI4/978-3-319-10605-2_54.html:text/html}
}

@article{mur-artal_orb-slam2:_2017,
	title = {{ORB}-{SLAM}2: {An} {Open}-{Source} {SLAM} {System} for {Monocular}, {Stereo}, and {RGB}-{D} 
{Cameras}},
	volume = {33},
	issn = {1552-3098},
	shorttitle = {{ORB}-{SLAM}2},
	doi = {10.1109/TRO.2017.2705103},
	abstract = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for 
monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The 
system works in real time on standard central processing units in a wide variety of environments from small 
hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our 
back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory 
estimation with metric scale. Our system includes a lightweight localization mode that leverages visual 
odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The 
evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in 
most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM 
community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Mur-Artal, R. and Tardós, J. D.},
	month = oct,
	year = {2017},
	keywords = {Feature extraction, mobile robots, robot vision, Simultaneous localization and mapping, 
SLAM (robots), Trajectory, Cameras, Optimization, simultaneous localization and mapping (SLAM), cameras, 
ORB-SLAM, distance measurement, Kalman filters, lightweight localization mode, Localization, map points, 
mapping, monocular cameras, motion estimation, open-source SLAM system, path planning, RGB-D, RGB-D cameras, 
simultaneous localization and mapping system, SLAM community, stereo, stereo cameras, Tracking loops, 
zero-drift localization},
	pages = {1255--1262},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/VPKUCTDG/7946260.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/59L3WGMD/Mur-Artal and Tardós - 2017 - 
ORB-SLAM2 An Open-Source SLAM System for Monocula.pdf:application/pdf}
}

@inproceedings{caselitz_monocular_2016,
	title = {Monocular camera localization in 3D {LiDAR} maps},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759304/},
	doi = {10.1109/IROS.2016.7759304},
	abstract = {Localizing a camera in a given map is essential for vision-based navigation. In contrast 
to common methods for visual localization that use maps acquired with cameras, we propose a novel approach, 
which tracks the pose of monocular camera with respect to a given 3D LiDAR map. We employ a visual odometry 
system based on local bundle adjustment to reconstruct a sparse set of 3D points from image features. These 
points are continuously matched against the map to track the camera pose in an online fashion. Our approach to 
visual localization has several advantages. Since it only relies on matching geometry, it is robust to changes 
in the photometric appearance of the environment. Utilizing panoramic LiDAR maps additionally provides 
viewpoint invariance. Yet lowcost and lightweight camera sensors are used for tracking. We present real-world 
experiments demonstrating that our method accurately estimates the 6-DoF camera pose over long trajectories 
and under varying conditions.},
	language = {en},
	urldate = {2018-03-27},
	publisher = {IEEE},
	author = {Caselitz, Tim and Steder, Bastian and Ruhnke, Michael and Burgard, Wolfram},
	month = oct,
	year = {2016},
	pages = {1926--1931},
	file = {Caselitz et al. - 2016 - Monocular camera localization in 3D LiDAR 
maps.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/CEL7IHIH/Caselitz et al. - 2016 - Monocular 
camera localization in 3D LiDAR maps.pdf:application/pdf}
}

@inproceedings{wolcott_visual_2014,
	title = {Visual localization within {LIDAR} maps for automated urban driving},
	isbn = {978-1-4799-6934-0 978-1-4799-6931-9},
	url = {http://ieeexplore.ieee.org/document/6942558/},
	doi = {10.1109/IROS.2014.6942558},
	abstract = {This paper reports on the problem of map-based visual localization in urban environments 
for autonomous vehicles. Self-driving cars have become a reality on roadways and are going to be a consumer 
product in the near future. One of the most signiﬁcant road-blocks to autonomous vehicles is the prohibitive 
cost of the sensor suites necessary for localization. The most common sensor on these platforms, a 
three-dimensional (3D) light detection and ranging (LIDAR) scanner, generates dense point clouds with measures 
of surface reﬂectivity—which other state-of-the-art localization methods have shown are capable of 
centimeter-level accuracy. Alternatively, we seek to obtain comparable localization accuracy with signiﬁcantly 
cheaper, commodity cameras. We propose to localize a single monocular camera within a 3D prior groundmap, 
generated by a survey vehicle equipped with 3D LIDAR scanners. To do so, we exploit a graphics processing unit 
to generate several synthetic views of our belief environment. We then seek to maximize the normalized mutual 
information between our real camera measurements and these synthetic views. Results are shown for two 
different datasets, a 3.0 km and a 1.5 km trajectory, where we also compare against the state-of-the-art in 
LIDAR map-based localization.},
	language = {en},
	urldate = {2018-03-27},
	publisher = {IEEE},
	author = {Wolcott, Ryan W. and Eustice, Ryan M.},
	month = sep,
	year = {2014},
	pages = {176--183},
	file = {Wolcott and Eustice - 2014 - Visual localization within LIDAR maps for 
automate.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/QF64SI42/Wolcott and Eustice - 2014 - 
Visual localization within LIDAR maps for automate.pdf:application/pdf}
}

@inproceedings{chen_robust_2017,
	title = {Robust {SLAM} system based on monocular vision and {LiDAR} for robotic urban search and 
rescue},
	doi = {10.1109/SSRR.2017.8088138},
	abstract = {In this paper, we propose a monocular SLAM system for robotic urban search and rescue 
(USAR), based on which most USAR tasks (e.g. localization, mapping, exploration and object recognition) can be 
fulfilled by rescue robots with only a single camera. The proposed system can be a promising basis to 
implement fully autonomous rescue robots. However, the feature-based map built by the monocular SLAM is 
difficult for the operator to understand and use. We therefore combine the monocular SLAM with a 2D LIDAR SLAM 
to realize a 2D mapping and 6D localization SLAM system which can not only obtain a real scale of the 
environment and make the map more friendly to users, but also solve the problem that the robot pose cannot be 
tracked by the 2D LIDAR SLAM when the robot climbing stairs and ramps. We test our system using a real rescue 
robot in simulated disaster environments. The experimental results show that good performance can be achieved 
using the proposed system in the USAR. The system has also been successfully applied and tested in the RoboCup 
Rescue Robot League (RRL) competitions, where our rescue robot team entered the top 5 and won the Best in 
Class small robot mobility in 2016 RoboCup RRL Leipzig Germany, and the champions of 2016 and 2017 RoboCup 
China Open RRL competitions.},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Safety}, {Security} and {Rescue} {Robotics} 
({SSRR})},
	author = {Chen, X. and Zhang, H. and Lu, H. and Xiao, J. and Qiu, Q. and Li, Y.},
	month = oct,
	year = {2017},
	keywords = {Feature extraction, mobile robots, robot vision, Simultaneous localization and mapping, 
SLAM (robots), monocular vision, 2D LIDAR SLAM, 6D localization SLAM system, disasters, fully autonomous 
rescue robots, Laser radar, LiDAR SLAM, monocular SLAM, monocular SLAM system, multi-robot systems, object 
recognition, Object recognition, relocalization, rescue robot team, rescue robots, Rescue robots, RoboCup 
Rescue Robot League competitions, robot climbing stairs, robot pose, robotic urban search, robust SLAM system, 
service robots, small robot mobility, Two dimensional displays, Urban search and rescue, USAR tasks},
	pages = {41--47}
}

@inproceedings{levinson_map-based_2007,
	title = {Map-{Based} {Precision} {Vehicle} {Localization} in {Urban} {Environments}},
	isbn = {978-0-262-52484-1},
	url = {http://www.roboticsproceedings.org/rss03/p16.pdf},
	doi = {10.15607/RSS.2007.III.016},
	abstract = {Many urban navigation applications (e.g., autonomous navigation, driver assistance 
systems) can beneﬁt greatly from localization with centimeter accuracy. Yet such accuracy cannot be achieved 
reliably with GPS-based inertial guidance systems, speciﬁcally in urban settings.},
	language = {en},
	urldate = {2018-03-27},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Levinson, J. and Montemerlo, M. and Thrun, S.},
	month = jun,
	year = {2007},
	file = {Levinson et al. - 2007 - Map-Based Precision Vehicle Localization in Urban 
.pdf:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/A9AURFHW/Levinson et al. - 2007 - Map-Based 
Precision Vehicle Localization in Urban .pdf:application/pdf}
}

@article{sermanet_overfeat:_2013,
	title = {{OverFeat}: {Integrated} {Recognition}, {Localization} and {Detection} using {Convolutional} 
{Networks}},
	shorttitle = {{OverFeat}},
	url = {http://arxiv.org/abs/1312.6229},
	abstract = {We present an integrated framework for using Convolutional Networks for classification, 
localization and detection. We show how a multiscale and sliding window approach can be efficiently 
implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to 
predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase 
detection confidence. We show that different tasks can be learned simultaneously using a single shared 
network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual 
Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and 
classifications tasks. In post-competition work, we establish a new state of the art for the detection task. 
Finally, we release a feature extractor from our best model called OverFeat.},
	urldate = {2018-03-27},
	journal = {arXiv:1312.6229 [cs]},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and 
LeCun, Yann},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6229},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1312.6229 
PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/G2RIBP3F/Sermanet et al. - 2013 - OverFeat 
Integrated Recognition, Localization and.pdf:application/pdf}
}

@inproceedings{bay_surf:_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SURF}: {Speeded} {Up} {Robust} {Features}},
	isbn = {978-3-540-33832-1 978-3-540-33833-8},
	shorttitle = {{SURF}},
	url = {https://link.springer.com/chapter/10.1007/11744023_32},
	doi = {10.1007/11744023_32},
	abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector 
and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously 
proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and 
compared much faster.This is achieved by relying on integral images for image convolutions; by building on the 
strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for 
the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This 
leads to a combination of novel detection, description, and matching steps. The paper presents experimental 
results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object 
recognition application. Both show SURF’s strong performance.},
	language = {en},
	urldate = {2018-03-27},
	booktitle = {Computer {Vision} – {ECCV} 2006},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bay, Herbert and Tuytelaars, Tinne and Gool, Luc Van},
	month = may,
	year = {2006},
	pages = {404--417},
	file = {Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/XWPJJMZ9/Bay et al. - 
2006 - SURF Speeded Up Robust 
Features.pdf:application/pdf;Snapshot:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/FAJ8LMU6/10.html:text/html}
}

@inproceedings{lowe_object_1999,
	title = {Object recognition from local scale-invariant features},
	volume = {2},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image 
features. The features are invariant to image scaling, translation, and rotation, and partially invariant to 
illumination changes and affine or 3D projection. These features share similar properties with neurons in 
inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently 
detected through a staged filtering approach that identifies stable points in scale space. Image keys are 
created that allow for local geometric deformations by representing blurred image gradients in multiple 
orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method 
that identifies candidate object matches. Final verification of each match is achieved by finding a low 
residual least squares solution for the unknown model parameters. Experimental results show that robust object 
recognition can be achieved in cluttered partially occluded images with a computation time of under 2 
seconds},
	booktitle = {Proceedings of the {Seventh} {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Lowe, D. G.},
	year = {1999},
	keywords = {feature extraction, Lighting, image matching, Neurons, object recognition, Object 
recognition, 3D projection, blurred image gradients, candidate object matches, cluttered partially occluded 
images, computation time, computational geometry, Computer science, Electrical capacitance tomography, 
Filters, Image recognition, inferior temporal cortex, Layout, least squares approximations, local geometric 
deformations, local image features, local scale-invariant features, low residual least squares solution, 
multiple orientation planes, nearest neighbor indexing method, primate vision, Programmable logic arrays, 
Reactive power, robust object recognition, staged filtering approach, unknown model parameters},
	pages = {1150--1157 vol.2},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/AGRJWLKV/790410.html:text/html}
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting 
linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, 
we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly 
outperform existing feature sets for human detection. We study the influence of each stage of the computation 
on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial 
binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for 
good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we 
introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose 
variations and backgrounds.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} 
{Recognition} ({CVPR}'05)},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	keywords = {Testing, feature extraction, Humans, object detection, Robustness, object recognition, 
Object recognition, coarse spatial binning, contrast normalization, edge based descriptors, fine orientation 
binning, fine-scale gradients, gradient based descriptors, gradient methods, High performance computing, 
Histograms, histograms of oriented gradients, human detection, Image databases, Image edge detection, linear 
SVM, Object detection, overlapping descriptor, pedestrian database, robust visual object recognition, support 
vector machines, Support vector machines},
	pages = {886--893 vol. 1},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/AGLJKWFG/1467360.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/7GRP49FP/Dalal and Triggs - 2005 - 
Histograms of oriented gradients for human detecti.pdf:application/pdf}
}

@inproceedings{rublee_orb:_2011,
	title = {{ORB}: {An} efficient alternative to {SIFT} or {SURF}},
	shorttitle = {{ORB}},
	doi = {10.1109/ICCV.2011.6126544},
	abstract = {Feature matching is at the base of many computer vision problems, such as object 
recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. 
In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation 
invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude 
faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world 
applications, including object detection and patch-tracking on a smart phone.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Rublee, E. and Rabaud, V. and Konolige, K. and Bradski, G.},
	month = nov,
	year = {2011},
	keywords = {object detection, image matching, binary descriptor, feature matching, object recognition, 
Boats, BRIEF, computer vision, noise resistance, ORB, patch-tracking, SIFT, smart phone, SURF, tracking, 
transforms},
	pages = {2564--2571},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/URPH74YZ/6126544.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/MII6WPYC/Rublee et al. - 2011 - ORB An 
efficient alternative to SIFT or SURF.pdf:application/pdf}
}

@inproceedings{levinson_robust_2010,
	title = {Robust vehicle localization in urban environments using probabilistic maps},
	doi = {10.1109/ROBOT.2010.5509700},
	abstract = {Autonomous vehicle navigation in dynamic urban environments requires localization accuracy 
exceeding that available from GPS-based inertial guidance systems. We have shown previously that GPS, IMU, and 
LIDAR data can be used to generate a high-resolution infrared remittance ground map that can be subsequently 
used for localization. We now propose an extension to this approach that yields substantial improvements over 
previous work in vehicle localization, including higher precision, the ability to learn and improve maps over 
time, and increased robustness to environment changes and dynamic obstacles. Specifically, we model the 
environment, instead of as a spatial grid of fixed infrared remittance values, as a probabilistic grid whereby 
every cell is represented as its own gaussian distribution over remittance values. Subsequently, Bayesian 
inference is able to preferentially weight parts of the map most likely to be stationary and of consistent 
angular reflectivity, thereby reducing uncertainty and catastrophic errors. Furthermore, by using offline SLAM 
to align multiple passes of the same environment, possibly separated in time by days or even months, it is 
possible to build an increasingly robust understanding of the world that can be then exploited for 
localization. We validate the effectiveness of our approach by using these algorithms to localize our vehicle 
against probabilistic maps in various dynamic environments, achieving RMS accuracy in the 10cm-range and thus 
outperforming previous work. Importantly, this approach has enabled us to autonomously drive our vehicle for 
hundreds of miles in dense traffic on narrow urban roads which were formerly unnavigable with previous 
localization methods.},
	booktitle = {2010 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Levinson, J. and Thrun, S.},
	month = may,
	year = {2010},
	keywords = {Robustness, probability, SLAM, Navigation, Laser radar, autonomous vehicle navigation, 
Bayesian inference, Bayesian methods, dynamic urban environment, Gaussian distribution, Global Positioning 
System, GPS-based inertial guidance systems, high-resolution infrared remittance ground map, IMU, LIDAR, 
Mobile robots, navigation, probabilistic grid, probabilistic maps, Reflectivity, Remotely operated vehicles, 
road vehicles, robust vehicle localization, spatial grid, Vehicle dynamics},
	pages = {4372--4378},
	file = {IEEE Xplore Abstract 
Record:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/96JIMV4W/5509700.html:text/html;IEEE Xplore 
Full Text PDF:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/39PM43BQ/Levinson and Thrun - 2010 - 
Robust vehicle localization in urban environments .pdf:application/pdf}
}

@misc{satoshi_kagami_autonomous_2013,
	title = {Autonomous {Vehicle} {Navigation} by {Building} 3D {Map} and by {Detecting} {Human} 
{Trajectory} using {LIDAR} - {Semantic} {Scholar}},
	url = 
{/paper/Autonomous-Vehicle-Navigation-by-Building-3D-Map-by-Kagami-Thompson/81b14341e3e063d819d032b6ce0bc0be0917c867},
	abstract = {This paper describes an autonomous vehicle navigation system based on Velodyne LIDAR. The 
system mainly focusing on an autonomy at the car park, and it includes following functions, 1) odometory 
correction, 2) 3D map building, 3) localization, 4) detecting human trajectories as well as static obstacles, 
5) path planning, and 6) vehicle control to a given trajectory. All those functions are developed on ROS. Car 
park of 70x50[m] area is used for experiment and results are shown.},
	urldate = {2018-03-28},
	author = {{Satoshi Kagami} and {Simon Thompson, Ippei Samejima, Tsuyoshi Hamada, Shinpei Kato, Naotaka 
Hatao, Yuma Nihei, Takuro Egawa, Kazuya Takeda, Hiroshi Takemura, Hiroshi Mizoguchi}},
	year = {2013},
	file = 
{Snapshot:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/JTKYRN7D/81b14341e3e063d819d032b6ce0bc0be0917c867.html:text/html}
}

@article{sujiwo_monocular_2016,
	title = {Monocular {Vision}-{Based} {Localization} {Using} {ORB}-{SLAM} with {LIDAR}-{Aided} {Mapping} 
in {Real}-{World} {Robot} {Challenge}},
	volume = {28},
	url = {https://www.fujipress.jp/jrm/rb/robot002800040479/},
	doi = {10.20965/jrm.2016.p0479},
	abstract = {Title: Monocular Vision-Based Localization Using ORB-SLAM with LIDAR-Aided Mapping in 
Real-World Robot Challenge {\textbar} Keywords: visual localization, autonomous vehicle, field robotics, 
Tsukuba Challenge {\textbar} Author: Adi Sujiwo, Tomohito Ando, Eijiro Takeuchi, Yoshiki Ninomiya, and Masato 
Edahiro},
	number = {4},
	urldate = {2018-03-28},
	journal = {Journal of Robotics and Mechatronics},
	author = {Sujiwo, Adi and Ando, Tomohito and Takeuchi, Eijiro and Ninomiya, Yoshiki and Edahiro, 
Masato},
	month = aug,
	year = {2016},
	pages = {479--490},
	file = 
{Snapshot:/home/zach/.zotero/zotero/n6tqo7vy.default/zotero/storage/NRRMNLPE/robot002800040479.html:text/html}
}
